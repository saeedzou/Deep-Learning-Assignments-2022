{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "import json\n",
    "import string\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the COCO dataset\n",
    "!wget http://images.cocodataset.org/zips/train2017.zip\n",
    "!wget http://images.cocodataset.org/zips/val2017.zip\n",
    "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "\n",
    "# unzip the files\n",
    "!unzip train2017.zip\n",
    "!unzip val2017.zip\n",
    "!unzip annotations_trainval2017.zip\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_images(path, num_cols=3, num_rows=3):\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 12))\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_cols):\n",
    "            img = cv2.imread(path + random.choice(os.listdir(path)))\n",
    "            axes[i, j].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "            axes[i, j].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# plot some random images from the training set\n",
    "plot_random_images('train2017/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the size of the train2017 folder?\n",
    "print('Number of images in train2017 folder: ', len(os.listdir('train2017/')))\n",
    "# what is the size of the val2017 folder?\n",
    "print('Number of images in val2017 folder: ', len(os.listdir('val2017/')))\n",
    "# list the files in the annotations folder\n",
    "print('Files in annotations folder: ', os.listdir('annotations/'))\n",
    "# load the annotations file\n",
    "with open('annotations/instances_train2017.json') as f:\n",
    "    instances = json.load(f)\n",
    "# print the keys of the instances dictionary\n",
    "print('Keys of instances dictionary: ', instances.keys())\n",
    "# print the keys of the instances['categories'] dictionary\n",
    "print('Keys of instances[\\'categories\\'] dictionary: ', instances['categories'][0].keys())\n",
    "# print the keys of the instances['annotations'] dictionary\n",
    "print('Keys of instances[\\'annotations\\'] dictionary: ', instances['annotations'][0].keys())\n",
    "# print the keys of the instances['images'] dictionary\n",
    "print('Keys of instances[\\'images\\'] dictionary: ', instances['images'][0].keys())\n",
    "# print the number of categories\n",
    "print('Number of categories: ', len(instances['categories']))\n",
    "# captions_train2017.json \n",
    "with open('annotations/captions_train2017.json') as f:\n",
    "    captions = json.load(f)\n",
    "# print the keys of the captions dictionary\n",
    "print('Keys of captions dictionary: ', captions.keys())\n",
    "# print the keys of the captions['annotations'] dictionary\n",
    "print('Keys of captions[\\'annotations\\'] dictionary: ', captions['annotations'][0].keys())\n",
    "# print the keys of the captions['images'] dictionary\n",
    "print('Keys of captions[\\'images\\'] dictionary: ', captions['images'][0].keys())\n",
    "# print the number of images\n",
    "print('Number of images: ', len(captions['images']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_length(captions):\n",
    "    # find the maximum length of the captions\n",
    "    max_len = max(len(caption['caption'].split()) for caption in captions['annotations'])\n",
    "    print('Maximum length of the captions: ', max_len)\n",
    "    return max_len\n",
    "max_len = find_max_length(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the captions\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "# define the punctuation and the stopwords\n",
    "punctuation = string.punctuation\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_captions(captions, max_len):\n",
    "    # create a dictionary to store the captions\n",
    "    captions_dict = {cap['image_id']: [] for cap in captions['annotations']}\n",
    "    for caption in captions['annotations']:\n",
    "        # get the image id\n",
    "        image_id = caption['image_id']\n",
    "        # remove stopwords\n",
    "        caption = ' '.join([word for word in caption['caption'].split() if word.lower() not in stopwords])\n",
    "        # remove the punctuation\n",
    "        caption = caption.translate(str.maketrans('', '', string.punctuation))\n",
    "        # tokenize the caption and remove the numbers\n",
    "        tokens = [token.lower() for token in caption.split() if token.isalpha()]\n",
    "        # add the tokens to the dictionary\n",
    "        captions_dict[image_id].append(tokens)\n",
    "        # Add start and end tokens to each caption\n",
    "        captions_dict[image_id][-1].insert(0, '<start>')\n",
    "        # Calculate the padding\n",
    "        padding = max_len - len(captions_dict[image_id][-1])\n",
    "        # pad the captions so that they are all of the same length\n",
    "        captions_dict[image_id][-1].extend(['<pad>'] * padding)\n",
    "    return captions_dict\n",
    "\n",
    "# preprocess the captions\n",
    "captions_dict = preprocess_captions(captions, max_len)\n",
    "print(captions_dict[62443])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(captions_dict):\n",
    "    # create a counter object\n",
    "    counter = Counter(word for image_id in captions_dict for caption in captions_dict[image_id] for word in caption)\n",
    "    # get the most common words\n",
    "    most_common_words = counter.most_common()\n",
    "    # create a vocabulary\n",
    "    vocabulary = {word[0]: i + 1 for i, word in enumerate(most_common_words)}\n",
    "    return vocabulary\n",
    "\n",
    "vocabulary = create_vocabulary(captions_dict)\n",
    "# add <end> token to the vocabulary\n",
    "vocabulary['<end>'] = len(vocabulary) + 1\n",
    "print(\"Size of the vocabulary:\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_captions_to_integers(captions_dict, vocabulary):\n",
    "    captions_dict_int = {image_id: [[vocabulary[token] for token in caption] for caption in captions_dict[image_id]] for image_id in captions_dict}\n",
    "    return captions_dict_int\n",
    "\n",
    "captions_dict_int = convert_captions_to_integers(captions_dict, vocabulary)\n",
    "print(captions_dict_int[62443])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset class for image captioning using COCO dataset and preprocessing steps like above given annotations file path \n",
    "class ImageCaptioningDataset(Dataset):\n",
    "    \"\"\"\n",
    "    The class ImageCaptioningDataset is a custom dataset class that is used to read the images and captions from the COCO dataset.\n",
    "\n",
    "    Args:\n",
    "        root_dir (string): The path to the root directory of the COCO dataset.\n",
    "        ann_file: the path to the annotations file that contains the captions for the images.\n",
    "        vocabulary: a dictionary that maps words to integers.\n",
    "        max_len: the maximum length of the captions.\n",
    "        transform: an optional PyTorch transform that is applied to the images before returning them.\n",
    "\n",
    "    Class Methods:\n",
    "        __init__(): This method is called when an object of the class is created. It loads the captions from the annotations\n",
    "        file and preprocesses them by removing stopwords, punctuations, and numbers and converts the captions to integers using the vocabulary. \n",
    "        It also saves the image ids in a list.\n",
    "\n",
    "        __len__(): This method returns the length of the dataset.\n",
    "\n",
    "        __getitem__(): This method returns the image and the caption for a given index.\n",
    "\n",
    "        preprocess_captions_and_convert_to_integers(): A helper function to preprocess captions and convert them to integers in one step.\n",
    "\n",
    "    Example:\n",
    "        >>> dataset = ImageCaptioningDataset(root_dir='train2017', ann_file='annotations/captions_train2017.json', vocabulary=vocabulary, max_len=max_len)\n",
    "        >>> print('Length of the dataset: ', len(dataset))\n",
    "        >>> image, caption = dataset[0]\n",
    "        >>> print('Image shape: ', image.shape)\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, ann_file, vocabulary, max_len, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.vocabulary = vocabulary\n",
    "        self.transform = transform\n",
    "        # Load the captions and preprocess them in one step using the provided functions\n",
    "        with open(ann_file) as f:\n",
    "            captions = json.load(f)\n",
    "        self.captions_dict_int = self.preprocess_captions_and_convert_to_integers(captions, vocabulary, max_len)\n",
    "        self.image_ids = list(self.captions_dict_int.keys())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # get the image id\n",
    "        image_id = self.image_ids[idx]\n",
    "        # get the image path (images are in the train2017 folder)\n",
    "        image_path = os.path.join(self.root_dir,  '%012d.jpg' % (image_id))\n",
    "        # read the image using PIL\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        # transform the image\n",
    "        if self.transform is not None:\n",
    "          image = self.transform(image)\n",
    "        # get the captions\n",
    "        captions = self.captions_dict_int[image_id]\n",
    "        # randomly select a caption\n",
    "        captions = random.choice(captions)\n",
    "        # convert the captions to a tensor\n",
    "        captions = torch.tensor(captions)\n",
    "        return image, captions\n",
    "    def preprocess_captions_and_convert_to_integers(self, captions, vocabulary, max_len):\n",
    "        # preprocess the captions\n",
    "        captions_dict = {cap['image_id']: [] for cap in captions['annotations']}\n",
    "        for caption in captions['annotations']:\n",
    "            # get the image id\n",
    "            image_id = caption['image_id']\n",
    "            # remove stopwords\n",
    "            caption = ' '.join([word for word in caption['caption'].split() if word.lower() not in stopwords])\n",
    "            # remove the punctuation\n",
    "            caption = caption.translate(str.maketrans('', '', string.punctuation))\n",
    "            # tokenize the caption and remove the numbers\n",
    "            tokens = [token.lower() for token in caption.split() if token.isalpha()]\n",
    "            # add the tokens to the dictionary\n",
    "            captions_dict[image_id].append(tokens)\n",
    "            # Add start and end tokens to each caption\n",
    "            captions_dict[image_id][-1].insert(0, '<start>')\n",
    "            # Calculate the padding\n",
    "            padding = max_len - len(captions_dict[image_id][-1])\n",
    "            # pad the captions so that they are all of the same length\n",
    "            captions_dict[image_id][-1].extend(['<pad>'] * padding)\n",
    "        # convert the captions to integers\n",
    "        captions_dict_int = {image_id: [[vocabulary[token] for token in caption] for caption in captions_dict[image_id]] for image_id in captions_dict}\n",
    "        return captions_dict_int\n",
    "\n",
    "\n",
    "\n",
    "# create the dataset\n",
    "root_dir = 'train2017'\n",
    "ann_file = 'annotations/captions_train2017.json'\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "dataset = ImageCaptioningDataset(root_dir, ann_file, vocabulary, transform=transform, max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show an image and its captions\n",
    "index = random.randint(0, len(dataset))\n",
    "image, captions = dataset[index]\n",
    "print('Image shape: ', image.shape)\n",
    "print('Captions shape: ', captions.shape)\n",
    "# convert the captions to a list of words remove the padding and start and end tokens\n",
    "captions = [list(vocabulary.keys())[list(vocabulary.values()).index(token)] for token in captions if token not in [vocabulary['<pad>'], vocabulary['<start>'], vocabulary['<end>']]]\n",
    "print(' '.join(captions))\n",
    "# show the image\n",
    "plt.imshow(image.permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataloader\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define A CNN-RNN model for image captioning in which the CNN is a pretrained ResNet-50 model(use timm library)\n",
    "# Each image is passed through the CNN and the output along with <start> token is passed to the RNN.\n",
    "# The RNN outputs the next word and the process is repeated until the <end> token is generated or the maximum length of the caption is reached.\n",
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        # encoder (CNN) - pretrained ResNet-50 model (pass output to another linear layer of size embedding_dim)\n",
    "        self.encoder = timm.create_model('resnet50', pretrained=True)\n",
    "        self.encoder.fc = nn.Linear(self.encoder.fc.in_features, embedding_dim)\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.encoder.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "        # decoder (RNN) - LSTM layer\n",
    "        self.decoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        # linear layer\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "    def forward(self, images, captions):\n",
    "        # pass the images through the encoder\n",
    "        features = self.encoder(images)\n",
    "        captions = self.embedding(captions)\n",
    "        # concatenate the features and captions\n",
    "        inputs = torch.cat((features.unsqueeze(1), captions), dim=1)\n",
    "        # pass the inputs through the decoder\n",
    "        outputs, _ = self.decoder(inputs)\n",
    "        # pass the outputs through the linear layer\n",
    "        outputs = self.linear(outputs)\n",
    "        return outputs\n",
    "# define the hyperparameters\n",
    "vocab_size = len(vocabulary)\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "model = ImageCaptioningModel(vocab_size, embedding_dim, hidden_dim)\n",
    "model = model.to(device)\n",
    "\n",
    "# define the loss function\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocabulary['<pad>'])\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model for 10 epochs\n",
    "# note that the model takes an image and passes it through the encoder to get the features\n",
    "# then it passes the features as the first input to the decoder, <start> token as the second input to the decoder and so on\n",
    "# the decoder outputs at first the <start> token and then the next word and so on\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, captions) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        print(captions.shape)\n",
    "        # add the <end> token to the captions\n",
    "        captions = torch.cat((captions, torch.ones((captions.shape[0], 1), dtype=torch.long).to(device) * vocabulary['<end>']), dim=1)\n",
    "        print(captions.shape)\n",
    "        outputs = model(images, captions[:, :-1])\n",
    "        print(outputs.shape)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs.reshape(-1, vocab_size), captions.reshape(-1))\n",
    "        print(outputs.reshape(-1, vocab_size).shape)\n",
    "        print(captions.reshape(-1).shape)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "        # print the loss\n",
    "        if i % 100 == 0:\n",
    "            print('Epoch: {}/{} | Step: {}/{} | Loss: {:.4f}'.format(epoch+1, num_epochs, i, len(dataloader), loss.item()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0 (tags/v3.9.0:9cf6752, Oct  5 2020, 15:34:40) [MSC v.1927 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fbe9694f7587329a2893969593bb646d9caf203732995a36644052b7dd475e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

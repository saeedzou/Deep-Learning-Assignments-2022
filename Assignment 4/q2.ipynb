{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the COCO dataset\n",
    "!wget http://images.cocodataset.org/zips/train2017.zip\n",
    "!wget http://images.cocodataset.org/zips/val2017.zip\n",
    "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "\n",
    "# unzip the files\n",
    "!unzip train2017.zip\n",
    "!unzip val2017.zip\n",
    "!unzip annotations_trainval2017.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# define the dataset\n",
    "trainset = torchvision.datasets.CocoDetection(root='./train2017', annFile='./annotations/instances_train2017.json', transform=transform)\n",
    "valset = torchvision.datasets.CocoDetection(root='./val2017', annFile='./annotations/instances_val2017.json', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 9 random images from the dataset\n",
    "def plot_random_images(dataset, num_cols=3, num_rows=3):\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 12))\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_cols):\n",
    "            index = np.random.randint(len(dataset))\n",
    "            image = dataset[index][0].numpy().transpose(1, 2, 0)\n",
    "            image = (image + 1) / 2\n",
    "            axes[i, j].imshow(image)\n",
    "            axes[i, j].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "plot_random_images(trainset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the size of the dataset?\n",
    "print('Size of the training dataset:', len(trainset))\n",
    "print('Size of the validation dataset:', len(valset))\n",
    "\n",
    "# what is the number of classes in the dataset? \n",
    "num_of_classes = len(trainset.coco.getCatIds())\n",
    "print('Number of classes in the dataset:', num_of_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does the captions_train2017.json file contain?\n",
    "import json\n",
    "with open('./annotations/captions_train2017.json') as f:\n",
    "    captions = json.load(f)\n",
    "print(captions.keys())\n",
    "print(captions['info'])\n",
    "print(captions['licenses'])\n",
    "print(captions['images'][0])\n",
    "print(captions['annotations'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the captions\n",
    "import string\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# define the punctuation and the stopwords\n",
    "punctuation = string.punctuation\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# define the function to preprocess the captions\n",
    "def preprocess_captions(captions):\n",
    "    # create a dictionary to store the captions\n",
    "    captions_dict = {}\n",
    "    # loop through the captions\n",
    "    for caption in captions['annotations']:\n",
    "        # get the image id\n",
    "        image_id = caption['image_id']\n",
    "        # get the caption\n",
    "        caption = caption['caption']\n",
    "        # remove the punctuation\n",
    "        caption = caption.translate(str.maketrans('', '', punctuation))\n",
    "        # tokenize the caption\n",
    "        tokens = word_tokenize(caption.lower())\n",
    "        # remove the stopwords\n",
    "        tokens = [token for token in tokens if token not in stopwords]\n",
    "        # remove the tokens with length less than 2\n",
    "        tokens = [token for token in tokens if len(token) > 1]\n",
    "        # remove the numbers\n",
    "        tokens = [token for token in tokens if token.isalpha()]\n",
    "        # add the tokens to the dictionary\n",
    "        if image_id not in captions_dict:\n",
    "            captions_dict[image_id] = []\n",
    "        captions_dict[image_id].append(tokens)\n",
    "    return captions_dict\n",
    "\n",
    "# preprocess the captions\n",
    "captions_dict = preprocess_captions(captions)\n",
    "print(captions_dict[62443])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add <start> and <end> tokens to the captions\n",
    "for image_id in captions_dict:\n",
    "    for caption in captions_dict[image_id]:\n",
    "        caption.insert(0, '<start>')\n",
    "        caption.append('<end>')\n",
    "print(captions_dict[62443])\n",
    "\n",
    "# What is the maximum length of the captions?\n",
    "max_length = 0\n",
    "for image_id in captions_dict:\n",
    "    for caption in captions_dict[image_id]:\n",
    "        max_length = max(max_length, len(caption))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add <pad> tokens to the captions\n",
    "for image_id in captions_dict:\n",
    "    for caption in captions_dict[image_id]:\n",
    "        while len(caption) < max_length:\n",
    "            caption.append('<pad>')\n",
    "print(captions_dict[62443])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vocabulary\n",
    "def create_vocabulary(captions_dict):\n",
    "    # create a list to store the words\n",
    "    words = []\n",
    "    # loop through the captions\n",
    "    for image_id in captions_dict:\n",
    "        for caption in captions_dict[image_id]:\n",
    "            words.extend(caption)\n",
    "    # create a counter object\n",
    "    counter = Counter(words)\n",
    "    # get the most common words\n",
    "    most_common_words = counter.most_common()\n",
    "    # create a vocabulary\n",
    "    vocabulary = {}\n",
    "    for i, word in enumerate(most_common_words):\n",
    "        vocabulary[word[0]] = i + 1\n",
    "    return vocabulary\n",
    "\n",
    "# create the vocabulary\n",
    "vocabulary = create_vocabulary(captions_dict)\n",
    "print(\"Size of the vocabulary:\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to convert the captions to integers\n",
    "def convert_captions_to_integers(captions_dict, vocabulary):\n",
    "    # create a dictionary to store the captions\n",
    "    captions_dict_int = {}\n",
    "    # loop through the captions\n",
    "    for image_id in captions_dict:\n",
    "        captions_dict_int[image_id] = []\n",
    "        for caption in captions_dict[image_id]:\n",
    "            caption_int = []\n",
    "            for token in caption:\n",
    "                caption_int.append(vocabulary[token])\n",
    "            captions_dict_int[image_id].append(caption_int)\n",
    "    return captions_dict_int\n",
    "\n",
    "# convert the captions to integers\n",
    "captions_dict_int = convert_captions_to_integers(captions_dict, vocabulary)\n",
    "print(captions_dict_int[62443])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to get the captions for a given image id\n",
    "def get_captions_for_image_id(captions_dict, image_id):\n",
    "    captions = []\n",
    "    for caption in captions_dict[image_id]:\n",
    "        caption = ' '.join(caption)\n",
    "        captions.append(caption)\n",
    "    return captions\n",
    "\n",
    "# get the captions for a given image id\n",
    "image_id = 62443\n",
    "captions = get_captions_for_image_id(captions_dict, image_id)\n",
    "print(captions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset class for image captioning using COCO dataset\n",
    "class ImageCaptioningDataset(Dataset):\n",
    "    def __init__(self, root_dir, ann_file, vocabulary, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.ann_file = ann_file\n",
    "        self.vocabulary = vocabulary\n",
    "        self.transform = transform\n",
    "        self.coco = COCO(ann_file)\n",
    "        self.ids = list(self.coco.anns.keys())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        coco = self.coco\n",
    "        vocabulary = self.vocabulary\n",
    "        ann_id = self.ids[index]\n",
    "        ann = coco.anns[ann_id]\n",
    "        img_id = ann['image_id']\n",
    "        caption = ann['caption']\n",
    "        \n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "        \n",
    "        image = Image.open(os.path.join(self.root_dir, path)).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # preprocess the caption\n",
    "        caption = caption.translate(str.maketrans('', '', string.punctuation))\n",
    "        tokens = word_tokenize(caption.lower())\n",
    "        tokens = [token for token in tokens if token not in stopwords]\n",
    "        tokens = [token for token in tokens if len(token) > 1]\n",
    "        tokens = [token for token in tokens if token.isalpha()]\n",
    "\n",
    "        # add <start> and <end> tokens to the caption\n",
    "        tokens.insert(0, '<start>')\n",
    "        tokens.append('<end>')\n",
    "\n",
    "        # convert the tokens to integers\n",
    "        caption = []\n",
    "        for token in tokens:\n",
    "            caption.append(vocabulary[token])\n",
    "\n",
    "        # pad the caption\n",
    "        while len(caption) < max_length:\n",
    "            caption.append(vocabulary['<pad>'])\n",
    "\n",
    "        # convert the caption to a tensor\n",
    "        caption = torch.tensor(caption)\n",
    "\n",
    "        return image, caption\n",
    "\n",
    "# create the dataset\n",
    "root_dir = 'train2017'\n",
    "ann_file = 'annotations/captions_train2017.json'\n",
    "dataset = ImageCaptioningDataset(root_dir, ann_file, vocabulary, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the image and caption for a given index\n",
    "index = 100\n",
    "image, caption = dataset[index]\n",
    "print(\"Caption:\", caption)\n",
    "\n",
    "idx2word = {v: k for k, v in vocabulary.items()}\n",
    "print(\"Caption:\", ' '.join([idx2word[i.item()] for i in caption if i not in [vocabulary['<start>'], vocabulary['<end>'], vocabulary['<pad>']]]))\n",
    "# create a function to display an image and its caption (remove <start> and <end> and <pad> tokens)\n",
    "def display_image_and_caption(image, caption):\n",
    "    # remove <start> and <end> and <pad> tokens\n",
    "    caption = [i for i in caption if i not in [vocabulary['<start>'], vocabulary['<end>'], vocabulary['<pad>']]]\n",
    "    # convert the caption to a string\n",
    "    caption = [idx2word[i.item()] for i in caption]\n",
    "    caption = ' '.join(caption)\n",
    "    # display the image and the caption\n",
    "    image = image.permute(1, 2, 0)\n",
    "    image = image.numpy()\n",
    "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
    "    image = image.clip(0, 1)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.title(caption)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# display the image and the caption\n",
    "display_image_and_caption(image, caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# create a function to collate the data\n",
    "def collate_fn(data):\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    # merge images (from tuple of 3D tensor to 4D tensor)\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # merge captions (from tuple of 1D tensor to 2D tensor)\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    captions = pad_sequence(captions, batch_first=True, padding_value=vocabulary['<pad>'])\n",
    "\n",
    "    return images, captions, lengths\n",
    "\n",
    "# create the data loader\n",
    "batch_size = 32\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fbe9694f7587329a2893969593bb646d9caf203732995a36644052b7dd475e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saeedzou/DeepLearning1401-01/blob/main/Assignment%204/q3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "fa8_1sCuXu-M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
        "import gc\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "!pip install -q transformers\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertModel\n",
        "!pip install -q hazm\n",
        "import hazm\n",
        "import os\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/aminrobatian/Persian_poems_corpus.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EokojhWYYGuI",
        "outputId": "7ed357f3-edf3-4149-cf1c-b03c048fd876"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Persian_poems_corpus'...\n",
            "remote: Enumerating objects: 159, done.\u001b[K\n",
            "remote: Total 159 (delta 0), reused 0 (delta 0), pack-reused 159\u001b[K\n",
            "Receiving objects: 100% (159/159), 45.21 MiB | 18.00 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "poets = ['parvin_norm.txt',\n",
        "         'shahriar_norm.txt',\n",
        "         'attar_norm.txt',\n",
        "         'farrokhi_norm.txt',\n",
        "         'saadi_norm.txt',\n",
        "         'bahar_norm.txt',\n",
        "         'jami_norm.txt',\n",
        "         'sanaee_norm.txt',\n",
        "         'moulavi_norm.txt',\n",
        "         'naserkhosro_norm.txt']"
      ],
      "metadata": {
        "id": "OD7vEa-0YLBN"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for poet in poets:\n",
        "  df = pd.read_csv(os.path.join('Persian_poems_corpus/normalized', poet), header=None, names=['text'])\n",
        "  if len(df) % 2 == 1:\n",
        "    df = df[:-1]\n",
        "  df = pd.DataFrame({'text': [df.iloc[i]['text'] + ' [SEP] ' + df.iloc[i+1]['text'] for i in range(0, len(df), 2)]})\n",
        "  data.append(df)\n",
        "result = pd.concat([df.assign(index=i) for i, df in enumerate(data)], axis=0, ignore_index=True)\n",
        "result = result.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "xaZ1LivpfEzL"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split data into train and val and test\n",
        "trainset = result.iloc[:int(len(result)*0.8)].values\n",
        "valset = result.iloc[int(len(result)*0.8):int(len(result)*0.9)].values\n",
        "testset = result.iloc[int(len(result)*0.9):].values\n",
        "# print the length of each dataset\n",
        "print('train: ', len(trainset))\n",
        "print('val: ', len(valset))\n",
        "print('test: ', len(testset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0evwTiCg4b3",
        "outputId": "d8ea1183-d76e-44d6-d6cf-97bc1fcc9cfa"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train:  197957\n",
            "val:  24745\n",
            "test:  24745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate max length of poems\n",
        "max_len = 0\n",
        "for poem in trainset:\n",
        "    if len(poem[0].split()) > max_len:\n",
        "        max_len = len(poem[0].split())\n",
        "print('max length of poems: ', max_len)\n",
        "# print the poem with max length\n",
        "print('poem with max length: ', trainset[np.argmax([len(poem[0].split()) for poem in trainset])][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVDrHi0CkUN6",
        "outputId": "7a82fdef-cb20-4844-b0ef-df5b47566b31"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max length of poems:  28\n",
            "poem with max length:  از ما اگر یکی می ماند  شیطان هزار می زاید و اضافه می شود [SEP] در کدام رویا می توانم ببینم  که یک از چنک هزار نجات یابد\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define a dataset class\n",
        "class PoemDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, tokenizer, max_len):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.texts = [tokenizer(text, padding='max_length', truncation=True, max_length=max_len, return_tensors='pt') for text in self.data[:, 0]]\n",
        "        self.labels = [poet for poet in self.data[:, 1]]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.texts[index], torch.tensor(self.labels[index]).long()"
      ],
      "metadata": {
        "id": "9yk27Gx7m-WM"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('HooshvareLab/bert-base-parsbert-uncased')\n",
        "train_dataset = PoemDataset(trainset, tokenizer, max_len)\n",
        "val_dataset = PoemDataset(valset, tokenizer, max_len)\n",
        "test_dataset = PoemDataset(testset, tokenizer, max_len)"
      ],
      "metadata": {
        "id": "WUKoIB3-pVYf"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "GfxxEi0jKEct"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ParsBERT = BertModel.from_pretrained('HooshvareLab/bert-fa-base-uncased')\n",
        "# freeze ParsBERT parameters\n",
        "for param in ParsBERT.parameters():\n",
        "    param.requires_grad = False\n",
        "# define a model class\n",
        "class PoemClassifier(nn.Module):\n",
        "    def __init__(self, ParsBERT, num_classes):\n",
        "        super(PoemClassifier, self).__init__()\n",
        "        self.ParsBERT = ParsBERT\n",
        "        self.classifier = nn.Linear(768, num_classes)\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.ParsBERT(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        output = self.classifier(output.pooler_output)\n",
        "        return output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0f_RNpUnznt",
        "outputId": "2aab9ec0-ae6c-4e3f-ff8a-d7ca4a451bae"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at HooshvareLab/bert-fa-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function for training\n",
        "def train(model, train_loader, val_loader, optimizer, criterion, epochs, device):\n",
        "    model = model.to(device)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_acc = 0\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids=inputs['input_ids'].squeeze(1).to(device), attention_mask=inputs['attention_mask'].to(device))\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "            train_acc += (outputs.argmax(1) == labels).sum().item()\n",
        "        train_loss /= len(train_loader)\n",
        "        train_acc /= len(train_loader.dataset)\n",
        "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "        print(f'Epoch: {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "    return train_loss, train_acc, val_loss, val_acc\n",
        "\n",
        "\n",
        "# define a function for evaluating\n",
        "def evaluate(model, val_loader, criterion, device):\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_acc = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(val_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(input_ids=inputs['input_ids'].squeeze(1).to(device), attention_mask=inputs['attention_mask'].to(device))\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            val_acc += (outputs.argmax(1) == labels).sum().item()\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc /= len(val_loader.dataset)\n",
        "    return val_loss, val_acc\n",
        "\n"
      ],
      "metadata": {
        "id": "k1KvGktrp_lL"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "model = PoemClassifier(ParsBERT, len(poets))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train_loss, train_acc, val_loss, val_acc = train(model, train_loader, val_loader, optimizer, criterion, 5, device)"
      ],
      "metadata": {
        "id": "wDhK6wFMqNTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model\n",
        "torch.save(model.state_dict(), 'poem_classifier.pt')\n",
        "# test the model\n",
        "test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')"
      ],
      "metadata": {
        "id": "X4oDXEmhtEeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "Ce5ITnUa1n6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function for predicting\n",
        "def predict(model, poem, tokenizer, max_len, device):\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    poem = tokenizer(poem, padding='max_length', truncation=True, max_length=max_len, return_tensors='pt')\n",
        "    poem = poem.to(device)\n",
        "    output = model(input_ids=poem['input_ids'].squeeze(1), attention_mask=poem['attention_mask'])\n",
        "    return output.argmax(1).item()\n",
        "\n",
        "# report the results, classification report and confusion matrix\n",
        "y_true = []\n",
        "y_pred = []\n",
        "for i, (inputs, labels) in enumerate(test_loader):\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(input_ids=inputs['input_ids'].squeeze(1).to(device), attention_mask=inputs['attention_mask'].to(device))\n",
        "    y_true.extend(labels.tolist())\n",
        "    y_pred.extend(outputs.argmax(1).tolist())\n",
        "print(classification_report(y_true, y_pred, target_names=poets))\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "print(f1_score(y_true, y_pred, average='macro'))"
      ],
      "metadata": {
        "id": "OT8u6P4I0JDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning with SGD and Adam"
      ],
      "metadata": {
        "id": "B8ouIoMa1zE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define another model where ParsBERT is not frozen\n",
        "ParsBERT = BertModel.from_pretrained('HooshvareLab/bert-fa-base-uncased')\n",
        "model = PoemClassifier(ParsBERT, len(poets))\n",
        "# train the model once with SGD optimizer and once with Adam optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# we want to measure perplexity before and after training. select 1000 unbiased samples from the test set\n",
        "# Get unique labels\n",
        "labels = np.unique(test[:, 1])\n",
        "# Initialize empty list to store samples\n",
        "samples = []\n",
        "# Loop over unique labels\n",
        "for label in labels:\n",
        "    # Get indices of samples with the current label\n",
        "    indices = np.where(test[:, 1] == label)[0]\n",
        "    # Randomly select 100 samples from those indices and get the corresponding samples\n",
        "    samples.append(test[np.random.choice(indices, 100, replace=False)])\n",
        "# Concatenate all samples into a single numpy array\n",
        "samples_dataset = PoemDataset(np.concatenate(samples))\n",
        "# Create a dataloader for the samples\n",
        "samples_loader = DataLoader(samples_dataset, batch_size=32, shuffle=True)\n",
        "# test the model\n",
        "test_loss, test_acc = evaluate(model, samples_loader, criterion, device)\n",
        "# calculate perplexity\n",
        "perplexity = np.exp(test_loss)\n",
        "print(f'Perplexity before training: {perplexity:.4f}')\n",
        "# train the model once with SGD optimizer\n",
        "train_loss, train_acc, val_loss, val_acc = train(model, train_loader, val_loader, optimizer, criterion, 10, device)\n",
        "# save the model\n",
        "torch.save(model.state_dict(), 'poem_classifier_sgd.pt')\n",
        "# test the model\n",
        "test_loss, test_acc = evaluate(model, samples_loader, criterion, device)\n",
        "# calculate perplexity\n",
        "perplexity = np.exp(test_loss)\n",
        "print(f'Perplexity after training: {perplexity:.4f}')\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "# report the results, classification report and confusion matrix\n",
        "y_true = []\n",
        "y_pred = []\n",
        "for i, (inputs, labels) in enumerate(test_loader):\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(input_ids=inputs['input_ids'].squeeze(1).to(device), attention_mask=inputs['attention_mask'].to(device))\n",
        "    y_true.extend(labels.tolist())\n",
        "    y_pred.extend(outputs.argmax(1).tolist())\n",
        "print(classification_report(y_true, y_pred, target_names=poets))\n",
        "print(confusion_matrix(y_true, y_pred))"
      ],
      "metadata": {
        "id": "l91L6Q1W1yZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define another model where ParsBERT is not frozen\n",
        "ParsBERT = BertModel.from_pretrained('HooshvareLab/bert-fa-base-uncased')\n",
        "model = PoemClassifier(ParsBERT, len(poets))\n",
        "# train the model once with SGD optimizer and once with Adam optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# we want to measure perplexity before and after training. select 1000 unbiased samples from the test set\n",
        "# test the model\n",
        "test_loss, test_acc = evaluate(model, samples_loader, criterion, device)\n",
        "# calculate perplexity\n",
        "perplexity = np.exp(test_loss)\n",
        "print(f'Perplexity before training: {perplexity:.4f}')\n",
        "# train the model once with Adam optimizer\n",
        "train_loss, train_acc, val_loss, val_acc = train(model, train_loader, val_loader, optimizer, criterion, 10, device)\n",
        "# save the model\n",
        "torch.save(model.state_dict(), 'poem_classifier_adam.pt')\n",
        "# test the model\n",
        "test_loss, test_acc = evaluate(model, samples_loader, criterion, device)\n",
        "# calculate perplexity\n",
        "perplexity = np.exp(test_loss)\n",
        "print(f'Perplexity after training: {perplexity:.4f}')\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "# report the results, classification report and confusion matrix\n",
        "y_true = []\n",
        "y_pred = []\n",
        "for i, (inputs, labels) in enumerate(test_loader):\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(input_ids=inputs['input_ids'].squeeze(1).to(device), attention_mask=inputs['attention_mask'].to(device))\n",
        "    y_true.extend(labels.tolist())\n",
        "    y_pred.extend(outputs.argmax(1).tolist())\n",
        "print(classification_report(y_true, y_pred, target_names=poets))\n",
        "print(confusion_matrix(y_true, y_pred))"
      ],
      "metadata": {
        "id": "97ZxnKhu12lb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
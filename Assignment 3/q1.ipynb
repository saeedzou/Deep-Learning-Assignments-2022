{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "from utils import calculate_metrics, count_parameters\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "writer = SummaryWriter('runs/q1/fc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the dataset and split it into train and test sets with a ratio of 70:30\n",
    "# transform normalize and resize the images to 150*150\n",
    "transform = transforms.Compose([transforms.Resize((150, 150)), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "dataset = torchvision.datasets.ImageFolder(root='Shoe vs Sandal vs Boot Dataset', transform=transform)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(num_images=1, root_dir=\"Shoe vs Sandal vs Boot Dataset\"):\n",
    "    classes = os.listdir(root_dir)\n",
    "    plt.figure(figsize=(len(classes) * 2, num_images * 2))\n",
    "    for i in range(len(classes)):\n",
    "        for j in range(num_images):\n",
    "            # get a random image from the class\n",
    "            img_name = np.random.choice(os.listdir(os.path.join(root_dir, classes[i])))\n",
    "            img_path = os.path.join(root_dir, classes[i], img_name)\n",
    "            img = plt.imread(img_path)\n",
    "            plt.subplot(num_images, len(classes), i * num_images + j + 1)\n",
    "            plt.imshow(img)\n",
    "            plt.title(classes[i])\n",
    "            plt.axis(\"off\")\n",
    "    plt.show()\n",
    "plot_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_layers):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_layers) - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_layers[i], hidden_layers[i + 1]))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        for i, linear in enumerate(self.hidden_layers):\n",
    "            if i != len(self.hidden_layers) - 1:\n",
    "                x = F.relu(linear(x))\n",
    "                x = self.dropout(x)\n",
    "            else:\n",
    "                x = self.softmax(linear(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to train on the train set and return the loss and accuracy of train and test sets\n",
    "# add the loss and accuracy to the tensorboard for each epoch\n",
    "def train(model, optimizer, train_loader, test_loader, epochs, criterion):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        test_loss = 0\n",
    "        train_correct = 0\n",
    "        test_correct = 0\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            train_correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        train_acc.append(train_correct / len(train_loader.dataset))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                output = model(images)\n",
    "                loss = criterion(output, labels)\n",
    "                test_loss += loss.item()\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                test_correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        test_losses.append(test_loss / len(test_loader))\n",
    "        test_acc.append(test_correct / len(test_loader.dataset))\n",
    "        print(f'Epoch: {epoch + 1}/{epochs}, '\n",
    "                f'Train loss: {train_loss / len(train_loader):.3f}, '\n",
    "                f'Train accuracy: {train_correct / len(train_loader.dataset):.3f}, '\n",
    "                f'Test loss: {test_loss / len(test_loader):.3f}, '\n",
    "                f'Test accuracy: {test_correct / len(test_loader.dataset):.3f}')\n",
    "        \n",
    "        writer.add_scalar('Loss/train', train_loss / len(train_loader), epoch)\n",
    "        writer.add_scalar('Loss/test', test_loss / len(test_loader), epoch)\n",
    "        writer.add_scalar('Accuracy/train', train_correct / len(train_loader.dataset), epoch)\n",
    "        writer.add_scalar('Accuracy/test', test_correct / len(test_loader.dataset), epoch)\n",
    "    return train_losses, test_losses, train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers = [150 * 150 * 3, 128, 64, 3]\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "fc_model = MLP(hidden_layers).to(device)\n",
    "# add the model to the tensorboard\n",
    "writer.add_graph(fc_model, torch.rand(1, 3, 150, 150).to(device))\n",
    "writer.close()\n",
    "\n",
    "optimizer = optim.Adam(fc_model.parameters(), lr=1e-4)\n",
    "train_loss, test_loss, train_acc, test_acc = train(fc_model, optimizer, train_loader, test_loader, epochs=10, criterion=criterion)\n",
    "# save the model\n",
    "torch.save(fc_model.state_dict(), \"fc_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The model has {count_parameters(fc_model):,} trainable parameters')    \n",
    "calculate_metrics(fc_model, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, block_dropout=True):\n",
    "        super(CNN, self).__init__()\n",
    "        self.block_dropout = block_dropout\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 18 * 18, 400)\n",
    "        self.fc2 = nn.Linear(400, 3)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(32)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(64)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(400)\n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(F.relu(self.batchnorm1(self.conv1(x))))\n",
    "        if self.block_dropout:\n",
    "            x = F.dropout2d(x, 0.2, training=self.training)\n",
    "        else:\n",
    "            x = F.dropout(x, 0.2, training=self.training)\n",
    "        x = self.maxpool(F.relu(self.batchnorm2(self.conv2(x))))\n",
    "        if self.block_dropout:\n",
    "            x = F.dropout2d(x, 0.2, training=self.training)\n",
    "        else:\n",
    "            x = F.dropout(x, 0.2, training=self.training)\n",
    "        x = self.maxpool(F.relu(self.batchnorm3(self.conv3(x))))\n",
    "        if self.block_dropout:\n",
    "            x = F.dropout2d(x, 0.2, training=self.training)\n",
    "        else:\n",
    "            x = F.dropout(x, 0.2, training=self.training)\n",
    "        x = x.view(-1, 64 * 18 * 18)\n",
    "        x = F.relu(self.batchnorm4(self.fc1(x)))\n",
    "        x = F.dropout(x, 0.2, training=self.training)\n",
    "        x = self.softmax(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model and move it to GPU and add to tensorboard\n",
    "writer = SummaryWriter(\"runs/q1_cnn_dropout\")\n",
    "cnn_model = CNN(block_dropout=False).to(device)\n",
    "writer.add_graph(cnn_model, torch.rand(1, 3, 150, 150).to(device))\n",
    "\n",
    "# define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=1e-4)\n",
    "\n",
    "# train the model\n",
    "cnn_train_losses, cnn_train_acc, cnn_test_losses, cnn_test_acc = train(cnn_model, optimizer, train_loader,test_loader, 10, criterion)\n",
    "# save the model\n",
    "torch.save(cnn_model.state_dict(), \"cnn_model_droput.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The model has {count_parameters(cnn_model):,} trainable parameters')\n",
    "calculate_metrics(cnn_model, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model and move it to GPU and add to tensorboard\n",
    "writer = SummaryWriter(\"runs/q1_cnn_block_dropout\")\n",
    "cnn_model_dropout = CNN(block_dropout=True).to(device)\n",
    "writer.add_graph(cnn_model_dropout, torch.rand(1, 3, 150, 150).to(device))\n",
    "\n",
    "# define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model_dropout.parameters(), lr=1e-4)\n",
    "\n",
    "# train the model\n",
    "cnn_train_losses, cnn_train_acc, cnn_test_losses, cnn_test_acc = train(cnn_model_dropout, optimizer, train_loader,test_loader, 10, criterion)\n",
    "# save the model\n",
    "torch.save(cnn_model_dropout.state_dict(), \"cnn_model_block_dropout.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The model has {count_parameters(cnn_model_dropout):,} trainable parameters')\n",
    "calculate_metrics(cnn_model_dropout, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1cbcb8df23fcabb14b98d08f167303b8612ee566ea031dd4a83c67bd8cdc07a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
